{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-based Control (Part 2)\n",
    "\n",
    "### Object Pose Estimation\n",
    "\n",
    "We will cover the following activities:\n",
    "1. Camera calibration\n",
    "2. Object pose estimation using fiducial markers (ArUco markers)\n",
    "3. Object pose estimation using ArUco board\n",
    "\n",
    "\n",
    "**Acknowledgements:** A good amount of this work builds on the tutorials provided by the **Robotics, Vision and Control** textbook Jupyter notebook [here](https://github.com/petercorke/RVC3-python/blob/d31ee173eab7acf43d58ee92f844289e34d279e4/notebooks/chap13.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Preamble (Install new dependencies)\n",
    "\n",
    "Make sure you have followed the instructions to install the requirement python packages. Please delete your old virtual environment, create a new one and run the pip install on the requirements.txt file again as new packages were added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import pi\n",
    "np.set_printoptions(\n",
    "    linewidth=120, formatter={\n",
    "        'float': lambda x: f\"{0:8.4g}\" if abs(x) < 1e-10 else f\"{x:8.4g}\"})\n",
    "np.random.seed(0)\n",
    "from machinevisiontoolbox.base import *\n",
    "from machinevisiontoolbox import *\n",
    "from spatialmath.base import *\n",
    "from spatialmath import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** \n",
    "\n",
    "Use the **camera_capture.py** script to capture at least 10 images of the calibration checkerboard in different orientations in the camera view. Note where the image files are stored. See image below for example:\n",
    "\n",
    "<img src=\"img\\ex-checkerboard.png\" width=\"500\">\n",
    "\n",
    "To find the right camera interface:\n",
    "\n",
    "1. On **Linux**, run ``v4l2-ctl --list-devices`` to get a list of all connected camera media devices\n",
    "2. On **Windows**, simplest way is to try 0, 1, 2, etc. until you find the right one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** \n",
    "\n",
    "Read the calibration images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ImageCollection(\"calibration_imgs/*.png\")\n",
    "\n",
    "id = 2\n",
    "images[id].disp() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**\n",
    "\n",
    "Compute the calibration results: camera intrinsic parameters (K), distortion, image frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, distortion, frames = CentralCamera.images2C(images, gridshape=(9, 6), squaresize=24e-3)\n",
    "\n",
    "print(f'The intrinsic matrix, K: \\n{K} \\n')\n",
    "print(f'The distortion parameters: \\n{distortion} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the distinction between the image center and the image principal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The image center is at {images[0].center}')\n",
    "print(f'The image principal point is at {K[0:2,2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display one of the image frames with the calibration corners drawn\n",
    "\n",
    "#  frames[id].image.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the estimated pose of the camera relative to each of the calibration images\n",
    "\n",
    "# for frame in frames:\n",
    "#     CentralCamera.plot(pose=frame.pose, scale=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:**\n",
    "\n",
    "Apply the intrinsic parameters and the lens distortion parameters to undistort the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract the intrinsic parameters\n",
    "\n",
    "# u0 = K[0,2]\n",
    "# v0 = ??\n",
    "# fpixel_width = ??\n",
    "# fpixel_height = ??\n",
    "\n",
    "\n",
    "# TODO: Extract the lens distortion parameters\n",
    "\n",
    "# k1, k2, p1, p2, k3 = distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert from pixel coordinates (u, v) to image plane coordinates (x, y)\n",
    "\n",
    "# U, V = images[id].meshgrid()\n",
    "\n",
    "# x = (U - u0) / fpixel_width\n",
    "# y = (V - v0) / fpixel_height\n",
    "\n",
    "\n",
    "# TODO: Calculate the radial distance of pixels from the principal point\n",
    "\n",
    "# r = np.sqrt(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the image coordinate errors due to both radial and tangential distortion\n",
    "\n",
    "# delta_x = x * (k1*r**2 + k2*r**4 + k3*r**6) + 2*p1*x*y + p2*(r**2 + 2*x**2)\n",
    "# delta_y = y * (k1*r**2 + k2*r**4 + k3*r**6) + p1*(r**2 + 2*y**2) + p2*x*y\n",
    "\n",
    "# TODO: distorted retinal coordinates\n",
    "\n",
    "# xd = x + delta_x \n",
    "# yd = y + delta_y\n",
    "\n",
    "# TODO: convert back from image coordinates to pixel coordinates in the distorted image\n",
    "\n",
    "# Ud = xd * fpixel_width + u0\n",
    "# Vd = yd * fpixel_height + v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the warp to a distorted image and observe the undistorted image\n",
    "\n",
    "# undistorted_img = images[id].warp(Ud, Vd)\n",
    "# undistorted_img.disp()\n",
    "# images[id].disp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Step:**\n",
    "\n",
    "To obtain the camera extrinsic parameters (i.e., the position and orientation of the camera frame in a world frame), we can use the known positions of the checkerboard corners and the intrinsic parameters and distortion as follows.\n",
    "\n",
    "The **goal of this code** is to compute the pose (position and orientation) and draw a 3D reference frame at the \"origin\" corner of the checkerboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(img, corners, imgpts):\n",
    "    \"\"\" TBD \"\"\"\n",
    "    corner = tuple(corners[0].ravel().astype(\"int32\"))\n",
    "    imgpts = imgpts.astype(\"int32\")\n",
    "    img = cv.line(img, corner, tuple(imgpts[0].ravel()), (255,0,0), 5)\n",
    "    img = cv.line(img, corner, tuple(imgpts[1].ravel()), (0,255,0), 5)\n",
    "    img = cv.line(img, corner, tuple(imgpts[2].ravel()), (0,0,255), 5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# add\n",
    "objpoints = np.zeros((9*6,3), np.float32)\n",
    "objpoints[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# add\n",
    "axis = np.float32([[2,0,0], [0,2,0], [0,0,-2]]).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the undistorted and convert to grayscale\n",
    "img = np.array(undistorted_img.bgr, dtype=np.uint8)\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find checkerboard corners in pixel coordinates\n",
    "ret, corners = cv.findChessboardCorners(gray, (9,6),None) \n",
    "\n",
    "if ret == True:\n",
    "    # Refine the corner positions in pixel coordinates\n",
    "    corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "    \n",
    "    # Given the known relative positions between objpoints and the corners2 (in pixels), \n",
    "    # find the rotation and translation between the checkerboard and the camera frame\n",
    "    ret, rvecs, tvecs = cv.solvePnP(objpoints, corners2, K, distortion)\n",
    "    \n",
    "    # project 3D points to image plane\n",
    "    imgpts, jac = cv.projectPoints(axis, rvecs, tvecs, K, distortion)\n",
    "\n",
    "    img = draw(img,corners2,imgpts)\n",
    "    plt.imshow(img)\n",
    "\n",
    "print(f\"Rotation vector: \\n{rvecs}\")\n",
    "print(f\"Translation vector: \\n{tvecs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the rotation vector to a rotation matrix using Rodrigues\n",
    "R, _ = cv.Rodrigues(rvecs)\n",
    "\n",
    "# Combine the 3x3 rotation matrix and the 3x1 translation vector to compute\n",
    "# the 4x4 camera transformation matrix\n",
    "T = np.hstack((R, tvecs.reshape(3, 1)))\n",
    "\n",
    "print(f\"Camera transformation matrix, T: \\n{T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice Exercise:**\n",
    "\n",
    "To build some intuition about the transformations between world, to camera to image to pixel coordinates and back, work on this exercise using the parameters we have computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Imagine an object at point (1.0, 0.0, 0.0), i.e., at one unit in the x-axis in the world frame\n",
    "# in our case, the reference frame of the checkerboard.\n",
    "obj_world = np.array([1, 0, 0, 1.0])\n",
    "\n",
    "# TODO: Project this object from the world frame to the camera frame using T\n",
    "# obj_camera = ??\n",
    "# print(f\"Object coordinate in the Camera Frame: {obj_camera[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Project the object from the 3D camera coordinates to the 2D image plane (in pixel coordinates) using K\n",
    "# obj_image = ??\n",
    "\n",
    "# TODO: Normalize by the z-coordinate to get homogeneous coordinates\n",
    "# obj_image = ??\n",
    "\n",
    "# print(f\"2D pixel coordinates: {obj_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify the transformations by drawing the object in the checkerboard and verifying the pixel coordinates\n",
    "\n",
    "# # Draw a circle at the 2D pixel coordinates\n",
    "# color = (200,100,200)\n",
    "# obj_pixel = (int(obj_image[0]), int(obj_image[1]))\n",
    "# cv.circle(img, obj_pixel, radius=6, color=color, thickness=-1)\n",
    "\n",
    "# # Draw the trace lines\n",
    "# cv.line(img, obj_pixel, (obj_pixel[0], 480), color, 1) # u axis\n",
    "# cv.line(img, obj_pixel, (0, obj_pixel[1]), color, 1) # v axis\n",
    "\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Object pose estimation using ArUco markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** \n",
    "\n",
    "Use the **camera_capture.py** script to capture an image of the ArUco marker. Note where the image files are stored. See image below for example:\n",
    "\n",
    "<img src=\"img\\ex-aruco-cubes.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:**\n",
    "\n",
    "Import the frame and then detect and perform pose estimation on the aruco markers in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the image frame stored from step 1\n",
    "scene = Image.Read(\"img/aruco-cubes.png\", rgb=False)\n",
    "scene.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Undistort the frame using the parameters derived during calibration\n",
    "\n",
    "# undistorted_scene = scene.warp(Ud, Vd)\n",
    "# undistorted_scene.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Detect and estimate pose of the markers in the frame\n",
    "# N.B.: the dictionary of the specific AruCo marker used, its size and the camera intrinsic parameters\n",
    "# are required\n",
    "\n",
    "# markers = undistorted_scene.fiducial(dict=\"36h11\", K=K, side=25e-3)\n",
    "\n",
    "# for marker in markers:\n",
    "#     print(marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: draw the pose frame in the image\n",
    "\n",
    "# for marker in markers:\n",
    "#   marker.draw(undistorted_scene, length=0.03, thick=3)\n",
    "# undistorted_scene.disp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Object pose estimation using ArUco board\n",
    "\n",
    "The reason we might want to do this is to avoid needing to attach markers on individual objects in our operating space and use their relative pose to the AruCo board to estimate their pose with respect to the camera frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:**\n",
    "\n",
    "Use the **camera_capture.py** script to capture an image of the ArUco board with NO objects on it. Note where the image files are stored. See image below for example:\n",
    "\n",
    "<img src=\"img\\ex-aruco-board.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:**\n",
    "\n",
    "Import the frame and then detect and perform pose estimation on the aruco board to determine the pose of the reference frame of the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = Image.Read(\"img/aruco-board.png\", rgb=False)\n",
    "scene.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Undistort the frame using the parameters derived during calibration\n",
    "\n",
    "# undistorted_scene = scene.warp(Ud, Vd)\n",
    "# undistorted_scene.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the aruco board object\n",
    "# N.B. take note of the parameters here\n",
    "\n",
    "# gridshape = (5, 7)\n",
    "# square_size = 28e-3\n",
    "# spacing_size = 3e-3\n",
    "# board = ArUcoBoard(gridshape, square_size, spacing_size, dict=\"6x6_1000\", firsttag=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the calibration parameters to create a calibrated camera object\n",
    "\n",
    "# C = np.column_stack((K, np.array([0, 0, 1]))) # camera matrix\n",
    "# est = CentralCamera.decomposeC(C)\n",
    "\n",
    "# camera = CentralCamera(f=est.f[0], rho=est.rho[0], imagesize=[480, 640], pp=est.pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: estimate the pose of the board frame wrt the camera frame\n",
    "\n",
    "# board.estimatePose(undistorted_scene, camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board.draw(undistorted_scene, camera, length=0.05, thick=4)\n",
    "# undistorted_scene.disp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**\n",
    "\n",
    "Follow the same steps as in **Step 2**, but in this case, place objects you would like to interact with on the board as in the image below.\n",
    "\n",
    "<img src=\"img\\ex-aruco-board-cubes.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is that you should be able to still determine the pose of the board frame (at the board origin) with respect to the camera frame and then using color-based image processing you can determine position (and possibly the orientation) of the target objects relative to the board (**this is intentionally vague as I want you to think deeply about this!**)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
